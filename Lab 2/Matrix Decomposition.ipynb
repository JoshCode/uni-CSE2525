{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, math\n",
    "\n",
    "RUN_TESTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function implementations\n",
    "## Non-negative Matrix Factorisation implementation\n",
    "### Provided design specification\n",
    "Implement the `nmf()` subroutine in the provided code base. This function takes as input a matrix `X`, the number of required components `n` (“number of features” from the lecture), a maximum number of iterations, and an error tolerance threshold. It returns two matrices `W` and `H` (with width/height `n`) such that `WH` approximates `X`.\n",
    "\n",
    "Use the algorithm from the lecture slides as the algorithm to compute `W` and `H`. For more information about it, you can read about it here.\n",
    "\n",
    "If at a certain point in the algorithm the reconstruction error of each consecutive iteration is less than `tol`, then you can stop early.\n",
    "\n",
    "`Hint: if at some place of the algorithm it's possible for a division by 0 to happen, add 1e-9 to the denominator.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    W = np.array([[2, 2, 2, 2], [4, 4, 4, 4]])\n",
    "    H = np.array([[1, 2, 3, 4], [4, 4, 2, 0]])\n",
    "    \n",
    "    e = 1e-15\n",
    "    print(e)\n",
    "    \n",
    "    print(W)\n",
    "    print(H)\n",
    "    print(W/(H+e))\n",
    "    print(W * H)\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-negative matrix factorisation implementation and tests\n",
    "\n",
    "RUN_TESTS = True\n",
    "\n",
    "def nmf(X: pd.DataFrame, n_components: int, max_iter: int=1000, tol: float=1e-3):\n",
    "    \"\"\"\n",
    "    Decomposes the original sparse matrix X into two matrices W and H. \n",
    "    \"\"\"\n",
    "    # Initialize W and H with random non-negative values\n",
    "    W = np.random.rand(X.shape[0], n_components)\n",
    "    H = np.random.rand(n_components, X.shape[1])\n",
    "\n",
    "    # START ANSWER\n",
    "    V = X.to_numpy()\n",
    "    e = 1e-99\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    prev_error = None\n",
    "    \n",
    "    error = V - (W @ H)\n",
    "    error = np.power(error, 2)\n",
    "    error = np.trace(error)\n",
    "    error = np.sqrt(error)\n",
    "    error_diff = np.inf\n",
    "    print(error_diff)\n",
    "    \n",
    "    while (error_diff > tol and iteration < max_iter):\n",
    "        numerator = V @ H.T\n",
    "        denominator = W @ H @ H.T\n",
    "        division = numerator / (denominator + e)\n",
    "        W = W * division\n",
    "\n",
    "        numerator = W.T @ V\n",
    "        denominator = W.T @ W @ H\n",
    "        division = numerator / (denominator + e)\n",
    "        H = H * division\n",
    "        \n",
    "        # Remember the error from previous iteration\n",
    "        prev_error = error\n",
    "        # Calculate reconstruction error\n",
    "        error = V - (W @ H)\n",
    "        error = np.power(error, 2)\n",
    "        error = np.trace(error)\n",
    "        error = np.sqrt(error)\n",
    "        # Error differential\n",
    "        error_diff = prev_error - error\n",
    "        iteration += 1\n",
    "        # TODO Fix error\n",
    "\n",
    "    print(f\"NMF optimized with {iteration} iterations and error of {error}\")\n",
    "    # V = X.to_numpy()\n",
    "    # i = 0\n",
    "    # E = np.inf\n",
    "    # Enew = np.linalg.norm(V - W@H, 'fro')**2\n",
    "    # print(E)\n",
    "    # print(Enew)\n",
    "    # while((E-Enew) > tol and i < max_iter): \n",
    "    #     i += 1\n",
    "    #     W *= (V@H.T) / (W@H@H.T + 1e-9)\n",
    "    #     H *= (W.T@V) / (W.T@W@H + 1e-9)\n",
    "\n",
    "    #     E = Enew\n",
    "    #     Enew = np.linalg.norm(V - W@H, 'fro')**2\n",
    "    #     Enew = np.sqrt(Enew)\n",
    "    # END ANSWER\n",
    "\n",
    "    return W, H\n",
    "\n",
    "if RUN_TESTS:\n",
    "    import unittest\n",
    "    \n",
    "    class TestSolution(unittest.TestCase):\n",
    "        def setUp(self):\n",
    "            np.random.seed(42)\n",
    "\n",
    "        def test_2_by_2(self):\n",
    "            col1 = [1, 1]\n",
    "            col2 = [0, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2)))\n",
    "            w, h = nmf(sparse_matrix, 4, 10)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False)\n",
    "\n",
    "        def test_3_by_3(self):\n",
    "            col1 = [1, 1, 0]\n",
    "            col2 = [0, 0, 0]\n",
    "            col3 = [0, 1, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2, col3)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "        def test_3_by_2(self):\n",
    "            col1 = [0, 1, 0]\n",
    "            col2 = [0, 0, 1]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "        def test_5_by_5(self):\n",
    "            col1 = [0, 1, 0, 0, 0]\n",
    "            col2 = [0, 0, 1, 1, 0]\n",
    "            col3 = [0, 0, 0, 0, 0]\n",
    "            col4 = [0, 1, 0, 0, 0]\n",
    "            col5 = [1, 0, 0, 0, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2, col3, col4, col5)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing implementation\n",
    "### Provided design specification\n",
    "Implement the `compute_signature()` subroutine in the provided code base. This function takes as input a list of `k` `HashFunction` and a list of `n` sets of integers, representing which `ids` each user has liked.\n",
    "\n",
    "Have a look in the library to see how `HashFunction` is defined.\n",
    "\n",
    "It should return the minhash signature for the given input, when applying the provided hash functions. The signature should be of size `k x n`, where each column of the signature matrix represents the index of the user’s liked ids, and the rows represent the index of each hash function.\n",
    "\n",
    "The goal is for similar sets of liked `ids` to have similar columns in the signature matrix. See the tests for an example of what’s expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [{1, 2, 3, 4}, {1}, {4, 5}, {1, 2, 3}, {1}]\n",
    "space = set().union(*ids)\n",
    "a = sorted(space)\n",
    "\n",
    "print(np.full((0, 0), sys.maxsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minhashing implementation and tests\n",
    "class HashFunction:\n",
    "    \"\"\"\n",
    "    Library class HashFunction. Do not change\n",
    "    This HashFunction class can be used to create an unique hash given an alpha and beta.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def hashf(self, x: float, n: int):\n",
    "        \"\"\"\n",
    "        Returns a hash given integers x and n.\n",
    "        :param x: The value to be hashed\n",
    "        :param n: The number of unique ids of all sets (modulo)\n",
    "        :return: The hashed value x given alpha and beta\n",
    "        \"\"\"\n",
    "        \n",
    "        hash_value = 0\n",
    "        hash_value =  (self.alpha * x + self.beta) % n\n",
    "        return hash_value\n",
    "\n",
    "def compute_signature(hashes: list[HashFunction], ids: list[set[int]]):\n",
    "    \"\"\"\n",
    "    This function will calculate the MinHash signature matrix from our sets of ids\n",
    "    using the list of hash functions (hashes)\n",
    "    :param hashes: The list of hash functions of arbitrary length\n",
    "    :param ids: The list of sets of ids\n",
    "    :return: The MinHash signature matrix for the given sets of ids\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.full((len(hashes), len(ids)), sys.maxsize)\n",
    "    space = set().union(*ids)\n",
    "    sorted_space = sorted(space)\n",
    "    \n",
    "    # START ANSWER\n",
    "    if len(hashes) == 0 or len(ids) == 0:\n",
    "        return np.full((len(hashes), len(ids)), sys.maxsize)\n",
    "    \n",
    "    max_id = max(sorted_space)\n",
    "    number_distinct_ids = len(sorted_space)\n",
    "    \n",
    "    # Initialise an existence matrix of max_id x number of id sets\n",
    "    # The matrix is 0-indexed, for index 0 matches `id` = 1\n",
    "    existence_matrix = np.full((number_distinct_ids, len(ids)), -1)\n",
    "\n",
    "    # Populate existence matrix\n",
    "    for i in range(0, existence_matrix.shape[0]):\n",
    "        for j in range(0, existence_matrix.shape[1]):\n",
    "            # Existence matrix entry (`i`, `j`) will be 1 if set `j` contains id (= i + 1)\n",
    "            # Else, it will be 0\n",
    "            id = sorted_space[i]\n",
    "            column_set = ids[j]\n",
    "            existence_matrix[i, j] = 1 if id in column_set else 0\n",
    "\n",
    "    # Calculate hash signature\n",
    "    for i in range(0, existence_matrix.shape[0]):\n",
    "        calculated_hashes = []\n",
    "        # First, we pre-calculate the hashes for the current row index `i`\n",
    "        for hashing_function in hashes:\n",
    "            calculated_hashes.append(hashing_function.hashf(i, number_distinct_ids))\n",
    "\n",
    "        # For every column in the existence matrix, if the entry is 1 (column j contains id i + 1)\n",
    "        # Update the hash signature for (`i`, `j`) if the new hash for row i is smaller than any previous hash\n",
    "        for j in range(0, existence_matrix.shape[1]):\n",
    "            if existence_matrix[i, j] == 1:\n",
    "                for result_i in range(0, result.shape[0]):\n",
    "                    result[result_i, j] = min(result[result_i, j], calculated_hashes[result_i])\n",
    "    # END ANSWER\n",
    "    return result\n",
    "\n",
    "if RUN_TESTS:\n",
    "    import unittest\n",
    "\n",
    "    class TestSolution(unittest.TestCase):\n",
    "\n",
    "        def test_multiple_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2, h3, h4]\n",
    "\n",
    "            test_sets = [{1, 2, 3, 4}, {1}, {4, 5}, {1, 2, 3}, {1}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[0, 3, 1, 0, 3],\n",
    "                                [0, 2, 3, 0, 2],\n",
    "                                [0, 3, 1, 0, 3],\n",
    "                                [0, 1, 0, 1, 1]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "        def test_identical_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2, h3, h4]\n",
    "\n",
    "            test_sets = [{2, 3}, {2, 3}, {2, 3}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[1, 1, 1],\n",
    "                                [0, 0, 0],\n",
    "                                [0, 0, 0],\n",
    "                                [0, 0, 0]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "        def test_mutually_exclusive_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "\n",
    "            test_hashes = [h1, h2, h3]\n",
    "\n",
    "            test_sets = [{1, 2}, {3, 4}, {5, 6}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[3, 1, 1],\n",
    "                                [0, 2, 0],\n",
    "                                [3, 0, 1]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "        \n",
    "        def test_non_consecutive_set(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2]\n",
    "\n",
    "            test_sets = [{2, 3, 6}, {2, 6}, {2, 3}, {3, 6}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[0, 0, 0, 1],\n",
    "                                 [0, 1, 0, 0]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data\n",
    "train_file_path = 'lab2_train.csv'\n",
    "test_file_path = 'lab2_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, delimiter=',')\n",
    "test_data = pd.read_csv(test_file_path, delimiter=',')\n",
    "\n",
    "# Clean up test_data labels\n",
    "train_data.rename(columns=str.strip, inplace=True)\n",
    "test_data.rename(columns=str.strip, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_repeated_pair(from_id, to_id, index):\n",
    "#     for (i, row) in train_data.iterrows():\n",
    "#         other_from_id = row['user_from_id']\n",
    "#         other_to_id = row['user_to_id']\n",
    "#         if from_id == other_from_id and other_to_id == to_id and not i == index:\n",
    "#             print(f\"Equality found: {i}: {other_from_id}, {other_to_id}\")\n",
    "\n",
    "# def check_reversed_pair(from_id, to_id):\n",
    "#     for (i, row) in train_data.iterrows():\n",
    "#         other_from_id = row['user_from_id']\n",
    "#         other_to_id = row['user_to_id']\n",
    "#         if from_id == other_to_id and other_from_id == to_id:\n",
    "#             print(f\"Equality found: {i}: {other_from_id}, {other_to_id}\")\n",
    "    \n",
    "\n",
    "# tuple_set = set()\n",
    "# tuple_dict = dict()\n",
    "# duplicate_set = set()\n",
    "# count = 0\n",
    "\n",
    "# for (i, row) in train_data.iterrows():\n",
    "#     from_id = row['user_from_id']\n",
    "#     to_id = row['user_to_id']\n",
    "    \n",
    "#     tuple = (from_id, to_id)\n",
    "#     if tuple in tuple_set:\n",
    "#         # print(f\"{i}: {from_id},{to_id}\")\n",
    "#         duplicate_set.add(tuple)\n",
    "#         count += 1\n",
    "#     tuple_set.add(tuple)\n",
    "    \n",
    "#     if tuple in tuple_dict:\n",
    "#         tuple_dict[tuple] =+ 1\n",
    "#     else:\n",
    "#         tuple_dict[tuple] = 0\n",
    "\n",
    "# print(count)\n",
    "# count = 0\n",
    "\n",
    "# for key in tuple_dict:\n",
    "#     if tuple_dict[key] > 0:\n",
    "#         count += 1\n",
    "        # print(key, tuple_dict[key])\n",
    "        \n",
    "# a, b, True, ---\n",
    "# a, b, False, ---\n",
    "        \n",
    "# print(count)\n",
    "\n",
    "\n",
    "# for tuple in duplicate_set:\n",
    "#     self_from_id = tuple[0]\n",
    "#     self_to_id = tuple[1]\n",
    "#     for (i, row) in train_data.iterrows():\n",
    "#         from_id = row['user_from_id']\n",
    "#         to_id = row['user_to_id']\n",
    "#         # is_like = row['is_like']\n",
    "#         # is_match = row['is_match']\n",
    "        \n",
    "#         print(f\"{i}: {from_id}, {to_id}\")\n",
    "#         check_repeated_pair(from_id, to_id, i)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         print(\"\")\n",
    "\n",
    "a=0\n",
    "b=0\n",
    "for i, row in train_data.iterrows():\n",
    "    if row['is_like']: a+=1\n",
    "    b+=1\n",
    "print(a, \"/\", b, \"=\", a/b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timo's notes about the data**\n",
    "\n",
    "**Practical information**  \n",
    "Data frame ID is equal to line number minus 2  \n",
    "The last two coluns of the data have the Object type, and can be of type bool or NaN  \n",
    "We have assumed people are not shown themselves\n",
    "\n",
    "\n",
    "**Hypothesis 1**: 'Mirrored' lines don't exist (two lines the same two people in opposite order)\n",
    "False: lines 172 and 7803\n",
    "\n",
    "**Hypothesis 2**: Mirrored lines exist, which represent the rare case two people are shown eachother at the same time  \n",
    "False: The two lines below contradict eachother:\n",
    "3476,1562,True,False\n",
    "1562,3476,True,True\n",
    "\n",
    "**Hypothesis 3**: A pair of people may be shown eachother more than once (and change their mind)  \n",
    "Note: Perfectly duplicate lines are possible  \n",
    "Note: Mirrored or duplicate lines tend to be sort of kinda far away from eachother (what is far?)\n",
    "\n",
    "**TO-DO:**  \n",
    "Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE2525 Data Mining: Lab 2 - Matrix Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new user-user matrix showing every posibility of a user being reccomended to another user\n",
    "n_users = 3716\n",
    "user_interactions = np.zeros((n_users+1,n_users+1))\n",
    "\n",
    "# Populate the matrix with likes and dislikes from the datas\n",
    "# The entries are assumed to be in temporal order, and the most recent entry is implicitly given priority in case of contradicting data (Because this probably represents a person changing their mind)\n",
    "def get_user_interactions(data):\n",
    "  for i, row in data.iterrows():\n",
    "    from_id = row['user_from_id']\n",
    "    to_id = row['user_to_id']\n",
    "    is_like = row['is_like']\n",
    "    is_match = row['is_match']\n",
    "\n",
    "    if(is_like):\n",
    "      user_interactions[from_id,to_id] = 1\n",
    "      # If user A got a match with user B, user B must have also liked user A. We log this, too\n",
    "      if(is_match): user_interactions[to_id,from_id] = 1\n",
    "    else: user_interactions[from_id,to_id] = -1\n",
    "\n",
    "  return user_interactions  \n",
    "\n",
    "user_interactions = get_user_interactions(train_data)\n",
    "\n",
    "# The bolow plot shows interactions. White dots are likes, and black dots are dislikes (grey means no information.)\n",
    "# No users have been removed. Even a user with a single dislike gives some useful information,\n",
    "# as the algorithm can't 'get away with' randomly assigning this user high values and concluding the user simply likes everyone.\n",
    "# While empty rows and columns provide no information and may as well be removed, there's little point to doing so.\n",
    "# These rows and columns don't affect other ones, as they are irrelevant to the reconstruction error.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(user_interactions,cmap='grey',\n",
    "interpolation='nearest', aspect='auto')\n",
    "plt.colorbar(label='Color scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nmf-algorithm (similar to above)\n",
    "def nmf(X, n_components, max_iter, tol):\n",
    "  # Eliminate negative values for nmf-algorithm (Likes are 1 here, dislikes/unknowns are 0)\n",
    "  V = np.where(X == 1, 1, 0)\n",
    "  # Create layermask indicating which values in the matrix are known (Likes/dislikes are 1 here, unknowns are 0)\n",
    "  layermask = np.where(X == 0, 0, 1)\n",
    "\n",
    "  W = np.random.rand(n_users+1, n_components)\n",
    "  H = np.random.rand(n_components, n_users+1)\n",
    "  i = 0\n",
    "  E = np.inf\n",
    "  Enew = np.linalg.norm((V - W@H)*layermask, 'fro') # Multiply difference matrix with layermask, so missing values don't affect reconstruction error\n",
    "  while(E-Enew > tol and i < max_iter): \n",
    "    i += 1\n",
    "    W *= (V@H.T) / (W@H@H.T + 1e-9) # No need to multiply V with the layermask here, unknown values in V are already set to 0\n",
    "    H *= (W.T@V) / (W.T@W@H + 1e-9)\n",
    "\n",
    "    E = Enew\n",
    "    Enew = np.linalg.norm((V - W@H)*layermask, 'fro')\n",
    "    print(\"Iteration\", i, \"complete\")\n",
    "  return W@H\n",
    "\n",
    "\n",
    "result_matrix = nmf(user_interactions, 5, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use result matrix to predict test data\n",
    "def apply_matrix(matrix, apply_to):\n",
    "  result = []\n",
    "  for i, row in apply_to.iterrows():\n",
    "    from_id = row['user_from_id']\n",
    "    to_id = row['user_to_id']\n",
    "\n",
    "    # Predict true iff the users are both known and they have a positive score\n",
    "    if(from_id>n_users or to_id>n_users or matrix[from_id, to_id]<=0.01): result.append(False)\n",
    "    else: result.append(True)\n",
    "  return result  \n",
    "\n",
    "x = apply_matrix(result_matrix,test_data)\n",
    "print(x)\n",
    "print(np.sum(x), \"/\", len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross-validate\n",
    "# n_chunks = 10\n",
    "# chunks = np.array_split(train_data, n_chunks)\n",
    "\n",
    "# for i in range(n_chunks):\n",
    "#     train_chunk = []\n",
    "#     # Select current test chunk and synthesize train chunk from remaining chunks\n",
    "#     for j, row in chunks.iterrows():\n",
    "#         if i==j: test_chunk = row\n",
    "#         else: train_chunk.append(row)\n",
    "#     # Apply all of the above code\n",
    "#     generated_matrix = nmf(get_user_interactions(train_chunk), 5, 100, 1e-3)\n",
    "#     result_list = apply_matrix(generated_matrix,test_chunk)\n",
    "#     score = 0\n",
    "#     for j, row in test_chunk:\n",
    "#         if row['is_like'] == result_list[j]: score+=1\n",
    "#     print(100*score/len(test_chunk),\"%\")    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
