{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import pdist, jaccard, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Default Python packages\n",
    "import sys\n",
    "from typing import Tuple, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom config class to globally change if we're debugging, running tests, running with fixed seeds\n",
    "class Config:\n",
    "    seed: int\n",
    "    run_tests = bool\n",
    "    fixed_seed: bool\n",
    "    do_print: bool\n",
    "\n",
    "Config.seed = 42\n",
    "Config.fixed_seed = True\n",
    "Config.run_tests = False\n",
    "Config.do_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Run All cells now before reading on\n",
    "\n",
    "## For peer-reviewing the report, scroll down to the H1 header \"Start of Report\"\n",
    "\n",
    "You can also collapse the headers **Ingest data** and **Function implementations** to get there quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data\n",
    "train_file_path = \"lab2_train.csv\"\n",
    "test_file_path = \"lab2_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, delimiter=\",\")\n",
    "test_data = pd.read_csv(test_file_path, delimiter=\",\")\n",
    "\n",
    "# Clean up test_data labels if necessary\n",
    "train_data.rename(columns=str.strip, inplace=True)\n",
    "test_data.rename(columns=str.strip, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function implementations\n",
    "\n",
    "The functions below are for reference only. The WebLab functions are to be graded. These may have changed to accomodate our pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorisation implementation\n",
    "### Provided design specification\n",
    "Implement the `nmf()` subroutine in the provided code base. This function takes as input a matrix `X`, the number of required components `n` (“number of features” from the lecture), a maximum number of iterations, and an error tolerance threshold. It returns two matrices `W` and `H` (with width/height `n`) such that `WH` approximates `X`.\n",
    "\n",
    "Use the algorithm from the lecture slides as the algorithm to compute `W` and `H`. For more information about it, you can read about it here.\n",
    "\n",
    "If at a certain point in the algorithm the reconstruction error of each consecutive iteration is less than `tol`, then you can stop early.\n",
    "\n",
    "`Hint: if at some place of the algorithm it's possible for a division by 0 to happen, add 1e-9 to the denominator.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-negative matrix factorisation implementation and tests\n",
    "\n",
    "RUN_TESTS = True\n",
    "\n",
    "def nmf(X: pd.DataFrame, n_components: int, max_iter: int=1000, tol: float=1e-3):\n",
    "    \"\"\"\n",
    "    Decomposes the original sparse matrix X into two matrices W and H. \n",
    "    \"\"\"\n",
    "    # Initialize W and H with random non-negative values\n",
    "    W = np.random.rand(X.shape[0], n_components)\n",
    "    H = np.random.rand(n_components, X.shape[1])\n",
    "\n",
    "    # START ANSWER\n",
    "    V = X.to_numpy()\n",
    "    e = 1e-99\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    prev_error = None\n",
    "    \n",
    "    error = V - (W @ H)\n",
    "    error = np.power(error, 2)\n",
    "    error = np.trace(error)\n",
    "    error = np.sqrt(error)\n",
    "    error_diff = np.inf\n",
    "    print(error_diff)\n",
    "    \n",
    "    while (error_diff > tol and iteration < max_iter):\n",
    "        numerator = V @ H.T\n",
    "        denominator = W @ H @ H.T\n",
    "        division = numerator / (denominator + e)\n",
    "        W = W * division\n",
    "\n",
    "        numerator = W.T @ V\n",
    "        denominator = W.T @ W @ H\n",
    "        division = numerator / (denominator + e)\n",
    "        H = H * division\n",
    "        \n",
    "        # Remember the error from previous iteration\n",
    "        prev_error = error\n",
    "        # Calculate reconstruction error\n",
    "        error = V - (W @ H)\n",
    "        error = np.power(error, 2)\n",
    "        error = np.trace(error)\n",
    "        error = np.sqrt(error)\n",
    "        # Error differential\n",
    "        error_diff = prev_error - error\n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"NMF optimized with {iteration} iterations and error of {error}\")\n",
    "    # V = X.to_numpy()\n",
    "    # i = 0\n",
    "    # E = np.inf\n",
    "    # Enew = np.linalg.norm(V - W@H, 'fro')**2\n",
    "    # print(E)\n",
    "    # print(Enew)\n",
    "    # while((E-Enew) > tol and i < max_iter): \n",
    "    #     i += 1\n",
    "    #     W *= (V@H.T) / (W@H@H.T + 1e-9)\n",
    "    #     H *= (W.T@V) / (W.T@W@H + 1e-9)\n",
    "\n",
    "    #     E = Enew\n",
    "    #     Enew = np.linalg.norm(V - W@H, 'fro')**2\n",
    "    #     Enew = np.sqrt(Enew)\n",
    "    # END ANSWER\n",
    "\n",
    "    return W, H\n",
    "\n",
    "if Config.run_tests:\n",
    "    import unittest\n",
    "    \n",
    "    class TestSolution(unittest.TestCase):\n",
    "        def setUp(self):\n",
    "            np.random.seed(42)\n",
    "\n",
    "        def test_2_by_2(self):\n",
    "            col1 = [1, 1]\n",
    "            col2 = [0, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2)))\n",
    "            w, h = nmf(sparse_matrix, 4, 10)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False)\n",
    "\n",
    "        def test_3_by_3(self):\n",
    "            col1 = [1, 1, 0]\n",
    "            col2 = [0, 0, 0]\n",
    "            col3 = [0, 1, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2, col3)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "        def test_3_by_2(self):\n",
    "            col1 = [0, 1, 0]\n",
    "            col2 = [0, 0, 1]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "        def test_5_by_5(self):\n",
    "            col1 = [0, 1, 0, 0, 0]\n",
    "            col2 = [0, 0, 1, 1, 0]\n",
    "            col3 = [0, 0, 0, 0, 0]\n",
    "            col4 = [0, 1, 0, 0, 0]\n",
    "            col5 = [1, 0, 0, 0, 0]\n",
    "            sparse_matrix = pd.DataFrame(list(zip(col1, col2, col3, col4, col5)))\n",
    "            w, h = nmf(sparse_matrix, 5, 50)\n",
    "            reconstructed_matrix = pd.DataFrame(data=np.dot(w, h),\n",
    "                                                index=sparse_matrix.index,\n",
    "                                                columns=sparse_matrix.columns)\n",
    "            pd.testing.assert_frame_equal(sparse_matrix, reconstructed_matrix, check_dtype=False, atol=0.05)\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing implementation\n",
    "### Provided design specification\n",
    "Implement the `compute_signature()` subroutine in the provided code base. This function takes as input a list of `k` `HashFunction` and a list of `n` sets of integers, representing which `ids` each user has liked.\n",
    "\n",
    "Have a look in the library to see how `HashFunction` is defined.\n",
    "\n",
    "It should return the minhash signature for the given input, when applying the provided hash functions. The signature should be of size `k x n`, where each column of the signature matrix represents the index of the user’s liked ids, and the rows represent the index of each hash function.\n",
    "\n",
    "The goal is for similar sets of liked `ids` to have similar columns in the signature matrix. See the tests for an example of what’s expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minhashing implementation and tests\n",
    "class HashFunction:\n",
    "    \"\"\"\n",
    "    Library class HashFunction. Do not change\n",
    "    This HashFunction class can be used to create an unique hash given an alpha and beta.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def hashf(self, x: float, n: int):\n",
    "        \"\"\"\n",
    "        Returns a hash given integers x and n.\n",
    "        :param x: The value to be hashed\n",
    "        :param n: The number of unique ids of all sets (modulo)\n",
    "        :return: The hashed value x given alpha and beta\n",
    "        \"\"\"\n",
    "        \n",
    "        hash_value = 0\n",
    "        hash_value = (self.alpha * x + self.beta) % n\n",
    "        return hash_value\n",
    "\n",
    "def compute_signature(hashes: list[HashFunction], ids: list[set[int]]):\n",
    "    \"\"\"\n",
    "    This function will calculate the MinHash signature matrix from our sets of ids\n",
    "    using the list of hash functions (hashes)\n",
    "    :param hashes: The list of hash functions of arbitrary length\n",
    "    :param ids: The list of sets of ids\n",
    "    :return: The MinHash signature matrix for the given sets of ids\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.full((len(hashes), len(ids)), sys.maxsize)\n",
    "    space = set().union(*ids)\n",
    "    sorted_space = sorted(space)\n",
    "    \n",
    "    # START ANSWER\n",
    "    if len(hashes) == 0 or len(ids) == 0:\n",
    "        return np.full((len(hashes), len(ids)), sys.maxsize)\n",
    "    \n",
    "    number_distinct_ids = len(sorted_space)\n",
    "    if Config.do_print: print(f\"number_distinct_ids={number_distinct_ids}\")\n",
    "    \n",
    "    # Initialise an existence matrix of max_id x number of id sets\n",
    "    # The matrix is 0-indexed, for index 0 matches `id` = 1\n",
    "    existence_matrix = np.full((number_distinct_ids, len(ids)), -1)\n",
    "\n",
    "    # Populate existence matrix\n",
    "    for i in range(0, existence_matrix.shape[0]):\n",
    "        for j in range(0, existence_matrix.shape[1]):\n",
    "            # Existence matrix entry (`i`, `j`) will be 1 if set `j` contains id (= i + 1)\n",
    "            # Else, it will be 0\n",
    "            id = sorted_space[i]\n",
    "            column_set = ids[j]\n",
    "            existence_matrix[i, j] = 1 if id in column_set else 0\n",
    "\n",
    "    # Calculate hash signature\n",
    "    for i in range(0, existence_matrix.shape[0]):\n",
    "        calculated_hashes = []\n",
    "        # First, we pre-calculate the hashes for the current row index `i`\n",
    "        for hashing_function in hashes:\n",
    "            calculated_hashes.append(hashing_function.hashf(i, number_distinct_ids))\n",
    "\n",
    "        # For every column in the existence matrix, if the entry is 1 (column j contains id i + 1)\n",
    "        # Update the hash signature for (`i`, `j`) if the new hash for row i is smaller than any previous hash\n",
    "        for j in range(0, existence_matrix.shape[1]):\n",
    "            if existence_matrix[i, j] == 1:\n",
    "                for result_i in range(0, result.shape[0]):\n",
    "                    result[result_i, j] = min(result[result_i, j], calculated_hashes[result_i])\n",
    "    # END ANSWER\n",
    "    return result\n",
    "\n",
    "if Config.run_tests:\n",
    "    import unittest\n",
    "\n",
    "    class TestSolution(unittest.TestCase):\n",
    "\n",
    "        def test_multiple_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2, h3, h4]\n",
    "\n",
    "            test_sets = [{1, 2, 3, 4}, {1}, {4, 5}, {1, 2, 3}, {1}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[0, 3, 1, 0, 3],\n",
    "                                [0, 2, 3, 0, 2],\n",
    "                                [0, 3, 1, 0, 3],\n",
    "                                [0, 1, 0, 1, 1]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "        def test_identical_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2, h3, h4]\n",
    "\n",
    "            test_sets = [{2, 3}, {2, 3}, {2, 3}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[1, 1, 1],\n",
    "                                [0, 0, 0],\n",
    "                                [0, 0, 0],\n",
    "                                [0, 0, 0]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "        def test_mutually_exclusive_sets(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "\n",
    "            test_hashes = [h1, h2, h3]\n",
    "\n",
    "            test_sets = [{1, 2}, {3, 4}, {5, 6}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[3, 1, 1],\n",
    "                                [0, 2, 0],\n",
    "                                [3, 0, 1]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "        \n",
    "        def test_non_consecutive_set(self):\n",
    "            h1 = HashFunction(2, 3)\n",
    "            h2 = HashFunction(4, 2)\n",
    "            h3 = HashFunction(1, 3)\n",
    "            h4 = HashFunction(3, 1)\n",
    "\n",
    "            test_hashes = [h1, h2]\n",
    "\n",
    "            test_sets = [{2, 3, 6}, {2, 6}, {2, 3}, {3, 6}]\n",
    "            \n",
    "            result = compute_signature(test_hashes, test_sets)\n",
    "            expected = np.array([[0, 0, 0, 1],\n",
    "                                 [0, 1, 0, 0]])\n",
    "            np.testing.assert_array_equal(result, expected)\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timo's notes about the data\n",
    "\n",
    "**Practical information:**\n",
    "Data frame ID is equal to line number minus 2\n",
    "The last two coluns of the data have the object type, and can be of type bool or NaN\n",
    "\n",
    "We have assumed people are not shown themselves\n",
    "\n",
    "\n",
    "**Hypothesis 1**: 'Mirrored' lines don't exist, where two lines exist of two people in the opposite order\n",
    "False: lines 172 and 7803\n",
    "\n",
    "**Hypothesis 2**: Mirrored lines exist, but in the rare case two people are shown eachother at the same time\n",
    "False: The two lines below contradict eachother:\n",
    "3476,1562,True,False\n",
    "1562,3476,True,True\n",
    "\n",
    "**Hypothesis 3**: People may be shown eachother more than once\n",
    "Note: Perfectly duplicate lines are possible\n",
    "Note: Mirrored or duplicate lines tend to be sort of kinda far away from eachother (what is far?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE2525 Data Mining: Lab 2 - Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of four columns: a directional user pair (`user_from_id`, `user_to_id`), a `is_like` boolean and a `is_match` boolean.\n",
    "\n",
    "We did some investgating of the data. Namely, testing a few assumptions and hypotheses. This was done with a lot of messy code, some of which is preserved at the very bottom of the file under the header **Data Exploration**\n",
    "\n",
    "**Hypothesis 1**: People are never shown themselves in the dating app, people can therefore never like themselves.  \n",
    "This turned out to be true.\n",
    "\n",
    "**Hypothesis 2**: 'Mirrored' lines don't exist, where two lines exist of two people in the opposite order  \n",
    "This turned out to be false, as an example: lines 172 and 7803 in the dataset\n",
    "\n",
    "**Hypothesis 3**: Mirrored lines exist, but then `like` and `match` should be the same.\n",
    "This also turned out to be false: The two lines below contradict eachother:\n",
    "3476,1562,True,False\n",
    "1562,3476,True,True\n",
    "\n",
    "**Hypothesis 4**: People may be shown eachother more than once  \n",
    "This is true!\n",
    "Note: Perfectly duplicate lines are possible\n",
    "Note: Mirrored or duplicate lines tend to be sort of kinda far away from eachother (what is far?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF-based recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NMF algorithm works by 'deconstructing' a matrix of liked objects and liking subjects into two matrices that attempt to represent the core properties that objects are made of and subjects find important. (whatever they may be.) In order to do that, we first need to generate this martix.\n",
    "\n",
    "In our case, the users like other users, so the matrix is of dimensions n_users,n_users - with one cell for each user's opinion of another user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new user-user matrix showing every posibility of a user being reccomended to another user\n",
    "n_users = 3716\n",
    "user_interactions = np.zeros((n_users+1,n_users+1))\n",
    "\n",
    "# Populate the matrix with likes and dislikes from the datas\n",
    "def get_user_interactions(data):\n",
    "  for i, row in data.iterrows():\n",
    "    from_id = row['user_from_id']\n",
    "    to_id = row['user_to_id']\n",
    "    is_like = row['is_like']\n",
    "    is_match = row['is_match']\n",
    "\n",
    "    if(is_like):\n",
    "      user_interactions[from_id,to_id] = 1\n",
    "      # If user A got a match with user B, user B must have also liked user A. We log this, too\n",
    "      if(is_match): user_interactions[to_id,from_id] = 1\n",
    "    else: user_interactions[from_id,to_id] = -1\n",
    "\n",
    "  return user_interactions  \n",
    "\n",
    "user_interactions = get_user_interactions(train_data)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(user_interactions,cmap='grey',\n",
    "interpolation='nearest', aspect='auto')\n",
    "plt.colorbar(label='Color scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows every interaction every user could have with another user. (Or themself) The white dots are likes, and black dots are dislikes. The grey zone are all interactions that haven't happened yet, so no information.\n",
    "\n",
    "Sometimes, the dataset contradicting itself. One entry would show user A likes user B, then another entry would show user A dislikes user B. We assumed each entry represents a user being shown to another user, and that this can happen more than once. We further assumed the entries are in temporal order. These contradictions then likely represent a user changing their mind, so the latest entry should be given precedence. This behavior is ensured in the code by simply having later entries override earlier ones.\n",
    "\n",
    "No users have been removed. Even a user with a single dislike gives some useful information, as the algorithm can't 'get away with' randomly assigning this user high values and concluding the user simply likes everyone. While empty rows and columns provide no information and may as well be removed, there's little point to doing so. These rows and columns don't affect other ones, as they are irrelevant to the reconstruction error. Removing them could perhaps have lead to a slight positive impact on memory and time use, but this would likely not even save enough time to make up for coding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nmf-algorithm (similar to above)\n",
    "def nmf(X, n_components, max_iter, tol):\n",
    "  # Eliminate negative values for nmf-algorithm (Likes are 1 here, dislikes/unknowns are 0)\n",
    "  V = np.where(X == 1, 1, 0)\n",
    "  # Create layermask indicating which values in the matrix are known (Likes/dislikes are 1 here, unknowns are 0)\n",
    "  layermask = np.where(X == 0, 0, 1)\n",
    "\n",
    "  W = np.random.rand(n_users+1, n_components)\n",
    "  H = np.random.rand(n_components, n_users+1)\n",
    "  i = 0\n",
    "  E = np.inf\n",
    "  Enew = np.linalg.norm((V - W@H)*layermask, 'fro') # Multiply difference matrix with layermask, so missing values don't affect reconstruction error\n",
    "  while(E-Enew > tol and i < max_iter): \n",
    "    i += 1\n",
    "    W *= (V@H.T) / (W@H@H.T + 1e-9) # No need to multiply V with the layermask here, unknown values in V are already set to 0\n",
    "    H *= (W.T@V) / (W.T@W@H + 1e-9)\n",
    "\n",
    "    E = Enew\n",
    "    Enew = np.linalg.norm((V - W@H)*layermask, 'fro')\n",
    "  return W@H\n",
    "\n",
    "\n",
    "result_matrix = nmf(user_interactions, 12, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the last line of the above cell. It contains the most relevant variables for the algorithm. These were balanced out experiamentally, and using 'manual' cross-validation. It would have been better to actually implement cross-validation, though. See the very bottom of this report for the corpse of our best attempt.\n",
    "\n",
    "We now created a user-to-user matrix with values predicting whether or not that user will like the other user - including the previusly missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(result_matrix,cmap='grey',\n",
    "interpolation='nearest', aspect='auto')\n",
    "plt.colorbar(label='Color scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the values of this matrix are all very low. This is likely because likes are much rarer than dislikes. The algorithm rightly assumes most interactions are likely to be dislikes.\n",
    "\n",
    "For this reason, we consider a value of 0.01 to be a reasonable recommendation threshold. This value was chosen to ensure the data suggests likes with the same frequency as they appear in the training set - about 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use result matrix to predict test data\n",
    "def apply_matrix(matrix, apply_to):\n",
    "  result = []\n",
    "  for i, row in apply_to.iterrows():\n",
    "    from_id = row['user_from_id']\n",
    "    to_id = row['user_to_id']\n",
    "\n",
    "    # Predict true iff the users are both known and they have a positive score\n",
    "    if(from_id>n_users or to_id>n_users or matrix[from_id, to_id]<=0.01): result.append(False)\n",
    "    else: result.append(True)\n",
    "  return result  \n",
    "\n",
    "prediction = apply_matrix(result_matrix,test_data)\n",
    "print(prediction)\n",
    "print(\"Frequency of likes predicted:\", np.sum(prediction), \"/\", len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an array representing our best attempt at predicting the missing values in the test set. The indices of the array correspond to the IDs of the entries they are a prediction for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance-based recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some supporting functions that wrangle our data into forms we want to use for minhashing.\n",
    "\n",
    "You can collapse the cell below for better readability of this section, we will cover what each function does when the pipeline gets to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting functions and classes for minhashing recommender\n",
    "# You can collapse this cell for better readability of the report\n",
    "class RandomHashFunction(HashFunction):\n",
    "    \"\"\"\n",
    "    UniversalHashFunction Library class HashFunction.\n",
    "    This HashFunction class can be used to create an unique hash given an alpha and beta.\n",
    "    \"\"\"\n",
    "\n",
    "    # BIG_PRIME is a large prime that should be less than max size for int32\n",
    "    BIG_PRIME = 799833737\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        generator = np.random.default_rng(seed)\n",
    "        self.alpha = generator.integers(low=1, high=self.BIG_PRIME - 1, dtype=np.int64)\n",
    "        self.beta = generator.integers(low=0, high=self.BIG_PRIME - 1, dtype=np.int64)\n",
    "\n",
    "    def hashf(self, x: float, n: int):\n",
    "        \"\"\"\n",
    "        Returns a hash given integers x and n.\n",
    "        :param x: The value to be hashed\n",
    "        :param n: The number of unique ids of all sets (modulo)\n",
    "        :return: The hashed value x given alpha and beta\n",
    "        \"\"\"\n",
    "        if n >= self.BIG_PRIME:\n",
    "            raise ValueError(\n",
    "                f\"Max value n ({n}) should be smaller than {self.BIG_PRIME}\"\n",
    "            )\n",
    "\n",
    "        return self._hashf(x) % n\n",
    "\n",
    "    def _hashf(self, x: float):\n",
    "        return (self.alpha * x + self.beta) % self.BIG_PRIME\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"RandomHashFunction(alpha={self.alpha}, beta={self.beta})\"\n",
    "\n",
    "def get_random_hash_functions(number_of_functions: int) -> list[HashFunction]:\n",
    "    result = []\n",
    "\n",
    "    incrementing_seed = None\n",
    "    if Config.fixed_seed:\n",
    "        incrementing_seed = Config.seed\n",
    "\n",
    "    for i in range(0, number_of_functions):\n",
    "        if Config.fixed_seed:\n",
    "            incrementing_seed += 1\n",
    "        result.append(RandomHashFunction(incrementing_seed))\n",
    "    return result\n",
    "\n",
    "def simple_jaccard(c1: np.ndarray, c2: np.ndarray) -> float:\n",
    "    intersect_size = np.intersect1d(c1, c2).size\n",
    "    union_size = np.union1d(c1, c2).size\n",
    "    if union_size == 0:\n",
    "        return 0\n",
    "    return intersect_size / union_size\n",
    "\n",
    "def print_k_neighbours(like_sets, user_ids, dists, k):\n",
    "    sorted_dists = zip(user_ids, dists)\n",
    "    sorted_dists = sorted(sorted_dists, key=lambda a: a[1], reverse=True)\n",
    "    \n",
    "    user_id_and_distance = sorted_dists.pop(0)\n",
    "    user_id = user_id_and_distance[0]\n",
    "    idx_user = user_ids.index(user_id)\n",
    "    user_like_set = np.array(list(like_sets[idx_user]))\n",
    "    print(f\"Likeset for user {user_id}\")\n",
    "    print(f\"  {user_id:>4} (dist={user_id_and_distance[1]:>6.2f}): {sorted(user_like_set.tolist())}\")\n",
    "    print(f\"Likesets for closest users\")\n",
    "    for i in range(k):\n",
    "        neighbour_id_and_distance = sorted_dists[i]\n",
    "        neighbour_id = neighbour_id_and_distance[0]\n",
    "        idx_neighbour = user_ids.index(neighbour_id)\n",
    "        neighbour_like_set = np.array(list(like_sets[idx_neighbour]))\n",
    "        jaccard_similarity = simple_jaccard(user_like_set, neighbour_like_set)\n",
    "        print(f\"  {neighbour_id:>4} (hashed_jaccard={neighbour_id_and_distance[1]:>6.2f}, true_jaccard={jaccard_similarity:>8.5f}):\")\n",
    "        print(f\"  {\"-\":>4} their likeset: {sorted(neighbour_like_set.tolist())}\")\n",
    "\n",
    "def plot(\n",
    "    df: pd.DataFrame,\n",
    "    window=None,\n",
    "    columns: list[str] = None,\n",
    "    title=\"Title\",\n",
    "    xcolumn: str = None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    figsize=(15, 8),\n",
    "    loc=None,\n",
    "    bbox_to_anchor=None,\n",
    "):\n",
    "    # If window is None, visualize the entire dataset\n",
    "    if window is None:\n",
    "        start_index, end_index = 0, len(df)\n",
    "    else:\n",
    "        start_index, end_index = window\n",
    "\n",
    "    # If sensors is None, select all columns\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "\n",
    "    if xcolumn is not None and xcolumn in columns:\n",
    "        columns = columns.drop[xcolumn]\n",
    "\n",
    "    # Plot each sensor in a different color\n",
    "    plt.figure(figsize=figsize)\n",
    "    for column in columns:\n",
    "        plt.plot(\n",
    "            df[xcolumn][start_index:end_index],\n",
    "            df[column][start_index:end_index],\n",
    "            label=column,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "    if loc is None:\n",
    "        if len(columns) < 5:\n",
    "            loc = \"best\"\n",
    "        else:\n",
    "            loc = \"upper right\"\n",
    "\n",
    "    if bbox_to_anchor is None:\n",
    "        if loc == \"best\":\n",
    "            bbox_to_anchor = (1.0, 1.0)\n",
    "        else:\n",
    "            bbox_to_anchor = (1.11, 1.0)\n",
    "\n",
    "    plt.legend(loc=loc, bbox_to_anchor=bbox_to_anchor)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def mh_get_likesets(df: pd.DataFrame) -> list[Tuple[int, set[int]]]:\n",
    "        max_id = -sys.maxsize\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            from_id = row[\"user_from_id\"]\n",
    "            to_id = row[\"user_to_id\"]\n",
    "\n",
    "            max_id = max(max_id, from_id)\n",
    "            max_id = max(max_id, to_id)\n",
    "        \n",
    "        max_id += 1\n",
    "\n",
    "        result: list[Tuple[int, set[int]]] = []\n",
    "        result = [(i, set()) for i in range(0, max_id)]\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            from_id = row[\"user_from_id\"]\n",
    "            to_id = row[\"user_to_id\"]\n",
    "            is_like: bool = row[\"is_like\"]\n",
    "            is_match: bool = row[\"is_match\"]\n",
    "\n",
    "            if is_like:\n",
    "                result[from_id][1].add(to_id)\n",
    "            # We decided to also check for matches, because it appears if there is a match, we don't have the like data point for the reverse pair\n",
    "            # Sidenote: if we do have the datapoint, it doesn't matter, because repeated entries cannot exist in sets anyway\n",
    "            if is_match:\n",
    "                result[to_id][1].add(from_id)\n",
    "\n",
    "        return result\n",
    "\n",
    "def mh_remove_empty_sets(user_ids, user_likes) -> None:\n",
    "        empty_indices = []\n",
    "        for i, likeset in enumerate(user_likes):\n",
    "            if len(likeset) == 0:\n",
    "                empty_indices.append(i)\n",
    "\n",
    "        if Config.do_print: print(f\"Will remove {len(empty_indices)} empty likesets\")\n",
    "\n",
    "        for i in sorted(empty_indices, reverse=True):\n",
    "            user_ids.pop(i)\n",
    "            user_likes.pop(i)\n",
    "\n",
    "        if Config.do_print: print(\n",
    "            f\"Resuming recommendation pipeline with {len(user_likes)} likesets for {len(user_ids)} users\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since minhashing is a way to approximate Jaccard similarity, we need to do some preprocessing to the data to get it into sets.\n",
    "\n",
    "We will consider the sets of likes for each user, called a ***likeset*** for a user. We will write a pipeline that computes *likeset* signatures through minhashing, and then we will calculate Jaccard similarities of each signature to all signatures to get a similarity matrix.\n",
    "\n",
    "Given a user $a$ with likeset $A$, the $k$ nearest neighbours of $a$: that is the users $b_k$ with the highest similarity in their likesets $B_k$ to $A$, will have some overlap in likes with user $a$ and some users that are not present in $A$. The users in the likesets of $a$'s nearest neighbours that are disjoint from $A$ are expected to be good recommendations to show to user $a$.\n",
    "\n",
    "Good recommendations are thus $R$, the set of users not liked by $a$ but liked by at least one of their $k$ nearest neighbours. Given by\n",
    "\n",
    "$$\n",
    "    R = \\left( \\bigcup_{i \\in \\{0 \\dots k\\}} B_i \\right) \\setminus A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want to get the set of users each user liked\n",
    "# user_ids_and_likes is a list of tuples: (int, set[int])\n",
    "# The tuples contain the user ID, and the set of all user IDs liked by this user in the provided dataset\n",
    "user_ids_and_likes = mh_get_likesets(train_data)\n",
    "\n",
    "# Next, we will unzip this list of tuples into two lists:\n",
    "# - user_ids is a list of user ids\n",
    "# - user_likes is a list of likesets\n",
    "# They share indices, so user_likes_sets[i] has the likeset for user_id user_likes_ids[i]\n",
    "user_ids, user_likes = map(list, (zip(*user_ids_and_likes)))\n",
    "\n",
    "if Config.do_print: print(f\"Aquired {len(user_likes)} likesets for {len(user_ids)} users\")\n",
    "\n",
    "# Some users have not liked anybody. This can result in weird recommendations for new users, or existing users that haven't liked anybody either.\n",
    "# We will remove all users that haven't liked anybody.\n",
    "mh_remove_empty_sets(user_ids, user_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset in a format we can use with minhashing (i.e. sets), we move on to getting the minhash signatures.  \n",
    "We define a number of hashing functions to be 500. We found this to be a good compromise between accuracy and speed.  \n",
    "In a future chapter, we will show examples to support this.\n",
    "\n",
    "The full pipeline runs in 9,8 seconds on our machine from raw in-memory data to finished distance matrix. Nearest neighbours and recommendations take a fraction of a second after that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get our random hashing functions.\n",
    "# They are defined as random initializations for the universal hashing function in the slides\n",
    "# hash(x, a, b) = ((ax + b) mod p) mod m\n",
    "# - a is a random number in range [1, p-1]\n",
    "# - a is a random number in range [0, p-1]\n",
    "# - p is a large prime number\n",
    "# - m is the maximum value for the hash code\n",
    "# in our case, p = 799833737\n",
    "number_of_hash_functions = 500\n",
    "hash_functions = get_random_hash_functions(number_of_hash_functions)\n",
    "hash_signatures = compute_signature(hash_functions, user_likes)\n",
    "\n",
    "if Config.do_print: print(f\"Aquired hash signatures for {len(user_ids)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we calculated the minhashes for our user columns. Now, we will calculate the Jaccard index, or Jaccard similarity, between every possible pair of hash columns. In other words, the similarity between ever pair or user hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard dissimilarity/distance with scipy pairwise distance\n",
    "hashed_jaccard_sim = pdist(hash_signatures.T, 'jaccard')\n",
    "\n",
    "# Distances are in lower triangular form, we want square form for ease of use.\n",
    "hashed_jaccard_sim = squareform(hashed_jaccard_sim)\n",
    "\n",
    "# We want the Jaccard simmilarity, not the Jaccard dissimilarity.\n",
    "# We can obtain this by simply taking the inverse.\n",
    "hashed_jaccard_sim = 1 - hashed_jaccard_sim\n",
    "print(f\"Calculated similarity matrix: {hashed_jaccard_sim.shape}\")\n",
    "\n",
    "# We should now have similarities in hashed_jaccard_sim\n",
    "# Where hashed_jaccard_sim[i][j] == hashed_jaccard_sim[j][i] == approximated Jaccard similarity of user i and user j likesets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated the Jaccard similarity of the hashed user likesets.  \n",
    "The Jaccard similarity of the hashed sets approximates the true Jaccard similarity between the original user likesets.\n",
    "\n",
    "To see this, we can look at an example user and compare their true Jaccard similarities to all likesets in the dataset with the approximated Jaccard similarity to all hashed likesets in the dataset. We will look at the user with ID `80`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_true_jaccard_in_hashed_distance_order(like_sets, user_ids, dists, plt_window=(0, 1000), plt_title=\"Correlation between true Jaccard and hashed Jaccard similarity\"):\n",
    "    sorted_dists = zip(user_ids, dists)\n",
    "    sorted_dists = sorted(sorted_dists, key=lambda a: a[1], reverse=True)\n",
    "    \n",
    "    # print(sorted_dists)\n",
    "\n",
    "    user_id_and_distance = sorted_dists.pop(0)\n",
    "    user_id = user_id_and_distance[0]\n",
    "    idx_user = user_ids.index(user_id)\n",
    "    user_like_set = np.array(list(like_sets[idx_user]))\n",
    "\n",
    "    data = {\"index\": [], \"id\": [], \"hash_jaccard\": [], \"true_jaccard\": []}\n",
    "    for i in range(len(sorted_dists)):\n",
    "        id_and_distance = sorted_dists[i]\n",
    "        id = id_and_distance[0]\n",
    "        hash_jaccard = id_and_distance[1]\n",
    "        idx_id = user_ids.index(id)\n",
    "        neighbour_like_set = np.array(list(like_sets[idx_id]))\n",
    "        true_jaccard = simple_jaccard(user_like_set, neighbour_like_set)\n",
    "\n",
    "        data[\"index\"].append(i)\n",
    "        data[\"id\"].append(id)\n",
    "        data[\"hash_jaccard\"].append(hash_jaccard)\n",
    "        data[\"true_jaccard\"].append(true_jaccard)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # print(df)\n",
    "    plot(\n",
    "        df,\n",
    "        window=plt_window,\n",
    "        columns=[\"true_jaccard\"],\n",
    "        title=plt_title,\n",
    "        xcolumn=\"index\",\n",
    "        xlabel=\"Datapoints sorted in decending order of hashed Jaccard similarity\",\n",
    "        ylabel=\"True Jaccard similarity\",\n",
    "        figsize=(10, 3)\n",
    "    )\n",
    "\n",
    "# Get the index of user ID 80 in the user_ids and user_likes lists\n",
    "idx_of_user_80 = user_ids.index(80)\n",
    "# Obtain the hashed Jaccard similarities of user 80 to all other hashed likesets\n",
    "user_80_dists = hashed_jaccard_sim[idx_of_user_80,:].tolist()\n",
    "\n",
    "# Graph the hashed Jaccard similarity in the order of high to low true Jaccard similarity\n",
    "graph_true_jaccard_in_hashed_distance_order(user_likes, user_ids, user_80_dists)\n",
    "print_k_neighbours(user_likes, user_ids, user_80_dists, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the graph, the true Jaccard similarity is graphed on the y-axis, with high-to-low sorted indices of hashed Jaccard similarity on the x-axis.  \n",
    "The first 5 neighbours of user 80 are also printed with their hashed_jaccard to user 80's hashed likeset, the true_jaccard between their unhashed, full likesets.  \n",
    "The full likesets are also printed, and from this we can see the overlapping values, and the non-overlapping values we may recommend to user 80 in the future.\n",
    "\n",
    "As we expect, a high Jaccard similarity to user 80's hashed likeset (left) corresponds to a high true Jaccard similarity of user 80's likeset and the same likeset as the hashed index with a high hashed similarity.\n",
    "\n",
    "As the Jaccard similarity of the hashed sets gets lower (left to right), the true Jaccard similarity between the same sets also declines, in general.\n",
    "\n",
    "There is some noise, this is to be expected because we're taking an approximation, and especially at the tail end, there are some `0.02` true Jaccard index datapoints which hash lower than other `0.00` true Jaccard index points.\n",
    "\n",
    "We are happy with this result though, because in a recommendation system, only the nearest neighbours really matter. From their likesets, we can take some unseen people and serve them to user 80. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the entire pipeline in a single function, mostly uncommented.\n",
    "# For the comments explaining the pipeline, please see the step-by-step above.\n",
    "def minhash_pipeline(data: pd.DataFrame, number_of_hash_functions: int = 500):\n",
    "    # user_ids_and_likes is a list of tuples: (int, set[int])\n",
    "    user_ids_and_likes = mh_get_likesets(data)\n",
    "\n",
    "    # user_ids is a list of user ids\n",
    "    # user_likes is a list of likesets\n",
    "    user_ids, user_likes = map(list, (zip(*user_ids_and_likes)))\n",
    "    if Config.do_print: print(f\"Aquired {len(user_likes)} likesets for {len(user_ids)} users\")\n",
    "    mh_remove_empty_sets(user_ids, user_likes)\n",
    "    \n",
    "    hash_functions = get_random_hash_functions(number_of_hash_functions)\n",
    "    hash_signatures = compute_signature(hash_functions, user_likes)\n",
    "\n",
    "    # strings = hash_signatures.T.tolist()\n",
    "    # strings = [f\"  {user_ids[i]:>4}: {elem}\" for (i, elem) in enumerate(strings)]\n",
    "    if Config.do_print: print(f\"Aquired hash signatures for {len(user_ids)} users\")\n",
    "    \n",
    "    hashed_jaccard_sim = pdist(hash_signatures.T, 'jaccard')\n",
    "    hashed_jaccard_sim = squareform(hashed_jaccard_sim)\n",
    "    hashed_jaccard_sim = 1 - hashed_jaccard_sim\n",
    "    if Config.do_print: print(f\"Calculated similarity matrix: {hashed_jaccard_sim.shape}\")\n",
    "    \n",
    "    return (user_ids, user_likes, hashed_jaccard_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show a few different settings for the number of hash functions and their results in the same type of graph as above, true Jaccard similarity of user 80 to all true sets plotted, on an x-axis in left-to-right high-to-low order of the hashed Jaccard similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_demo_number_of_hashes():\n",
    "    old_print_const = Config.do_print\n",
    "    Config.do_print = False\n",
    "    hash_numbers = [1, 5, 10, 50, 100, 500]\n",
    "    \n",
    "    for hash_number in hash_numbers:\n",
    "        start_time = datetime.now()\n",
    "        user_ids, user_likes, hashed_jaccard_sim = minhash_pipeline(train_data, hash_number)\n",
    "        end_time = datetime.now()\n",
    "        algo_time = end_time - start_time\n",
    "        algo_time = algo_time.total_seconds()\n",
    "        print(f\"{hash_number} hashes took {algo_time} seconds\")\n",
    "    \n",
    "        # Get the index of user ID 80 in the user_ids and user_likes lists\n",
    "        idx_of_user_80 = user_ids.index(80)\n",
    "        # Obtain the hashed Jaccard similarities of user 80 to all other hashed likesets\n",
    "        user_80_dists = hashed_jaccard_sim[idx_of_user_80,:].tolist()\n",
    "\n",
    "        # Graph the hashed Jaccard similarity in the order of high to low true Jaccard similarity\n",
    "        graph_true_jaccard_in_hashed_distance_order(user_likes, user_ids, user_80_dists, plt_title=f\"Jaccard similarity of user 80 with {hash_number} hashes\")\n",
    "    Config.do_print = old_print_const\n",
    "    \n",
    "mh_demo_number_of_hashes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible, with 1 hash the data is quite random, although there is some clustering of the high true hashes to the left already showing even with only 1 hash function.  \n",
    "As the number of hashes increase, the quality of the approximation also increases, ending up with the same graph for 500 hashes as the earlier graph for user 80.\n",
    "\n",
    "If we continue increasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_demo_number_of_hashes_higher():\n",
    "    old_print_const = Config.do_print\n",
    "    Config.do_print = False\n",
    "    hash_numbers = [500, 1000, 1500, 2000, 3000]\n",
    "    \n",
    "    for hash_number in hash_numbers:\n",
    "        start_time = datetime.now()\n",
    "        user_ids, user_likes, hashed_jaccard_sim = minhash_pipeline(train_data, hash_number)\n",
    "        end_time = datetime.now()\n",
    "        algo_time = end_time - start_time\n",
    "        algo_time = algo_time.total_seconds()\n",
    "        print(f\"{hash_number} hashes took {algo_time} seconds\")\n",
    "    \n",
    "        # Get the index of user ID 80 in the user_ids and user_likes lists\n",
    "        idx_of_user_80 = user_ids.index(80)\n",
    "        # Obtain the hashed Jaccard similarities of user 80 to all other hashed likesets\n",
    "        user_80_dists = hashed_jaccard_sim[idx_of_user_80,:].tolist()\n",
    "\n",
    "        # Graph the hashed Jaccard similarity in the order of high to low true Jaccard similarity\n",
    "        graph_true_jaccard_in_hashed_distance_order(user_likes, user_ids, user_80_dists, plt_title=f\"Jaccard similarity of user 80 with {hash_number} hashes\")\n",
    "    Config.do_print = old_print_const\n",
    "    \n",
    "mh_demo_number_of_hashes_higher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of hash functions goes up, so does the quality of the approximation.  \n",
    "\n",
    "At values higher than 500 though, it's quickly increasing the runtime of the algorithm to a point where it's not acceptable anymore, up to 38.23 seconds for 3000 hashes on our machine. At 3000 hashes, we're even increasing the resolution of our dataset, as we have about 2 400 non-empty likesets.\n",
    "\n",
    "We believe 500 hashes is a good sweet spot for low runtime and high enough accuracy of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our example user 80, the recommendations have been part of the printed code output above, but to highlight what's happening, we'll go through it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_get_recommendations(data: pd.DataFrame, user_id: int, k: int = 3):\n",
    "    # Run the pipeline\n",
    "    user_ids, user_likes, hashed_jaccard_sim = minhash_pipeline(data)\n",
    "    \n",
    "    # Get index of our user_id and distances for that user\n",
    "    idx_user_id = user_ids.index(user_id)\n",
    "    user_dists = hashed_jaccard_sim[idx_user_id,:].tolist()\n",
    "    \n",
    "    # Sort the distances in decreasing order of similarity (highly similar users: their likes are good recommendations)\n",
    "    sorted_user_dists = zip(user_ids, user_dists)\n",
    "    sorted_user_dists = sorted(sorted_user_dists, key=lambda a: a[1], reverse=True)\n",
    "    \n",
    "    # Nearest k neighbours are usually the range 0:k\n",
    "    # But, the user itself is also in the sorted distances, so we need to skip over index 0, which makes it 1:k+1\n",
    "    nearest_k_users = sorted_user_dists[1:k+1]\n",
    "    \n",
    "    if Config.do_print: print(f\"Retrieved {len(nearest_k_users)}-nearest users for user {user_id}\")\n",
    "    \n",
    "    # Get the set of likes for our user_id\n",
    "    user_like_set = np.array(list(user_likes[idx_user_id]))\n",
    "    \n",
    "    # Get the set of likes for our nearest-k\n",
    "    nearest_likes = set()\n",
    "    for i in range(len(nearest_k_users)):\n",
    "        neighbour_id_and_distance = sorted_user_dists[i]\n",
    "        neighbour_id = neighbour_id_and_distance[0]\n",
    "        idx_neighbour = user_ids.index(neighbour_id)\n",
    "        neighbour_like_set = np.array(list(user_likes[idx_neighbour]))\n",
    "        for like in neighbour_like_set:\n",
    "            nearest_likes.add(like)\n",
    "    \n",
    "    # Remove ids our user already liked\n",
    "    # Remaining are liked users for our nearest neighbours that our user hasn't liked yet\n",
    "    # These are good recommendations for our user, as they are more likely to also like them, because similar users to our user also liked these.\n",
    "    recommendations = nearest_likes.difference(user_like_set)\n",
    "    \n",
    "    print_k_neighbours(user_likes, user_ids, user_dists, 3)\n",
    "    \n",
    "    print()\n",
    "    print(\"===== Output =====\")\n",
    "    print(f\"Found {len(recommendations)} recommendations for user {user_id} from their {len(nearest_k_users)} nearest neighbours' likes.\")\n",
    "    print(f\"{len(nearest_likes)} neighbours' likes were under consideration, but {len(nearest_likes)-len(recommendations)} intersecting likes with user {user_id} were removed.\")\n",
    "    print(f\"The recommendations are:\")\n",
    "    print(f\" - {sorted(list(recommendations))}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "recommendations = mh_get_recommendations(train_data, user_id=80, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the output above, we found the same 3 nearest neighbours for user 80 as before: user 19, 36 and 93.\n",
    "\n",
    "As they are the most similar users to user 80, there is some overlap in their likes. Our recommendation set is the difference between (the union of neighbour sets) and user 80's likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new demo user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment also asks to give recommendations for a new user. To continue with our ongoing example, we will append a demo user to our data with a few overlapping likes with user 80. We expect the pipeline to find 80 as it's nearest neighbour, and recommend similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_get_recommendations_for_demo_user(data: pd.DataFrame, user_id: int, k: int = 3):\n",
    "    # --- Here we inject our new demo user ---\n",
    "    # We give a new user (id = 4444) 10 likes that user 80 has also liked\n",
    "    new_data = data.copy(deep=True)\n",
    "    new_data.loc[len(new_data)] = [4444, 2, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 6, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 29, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 50, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 58, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 90, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 156, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 192, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 194, True, False]\n",
    "    new_data.loc[len(new_data)] = [4444, 202, True, False]\n",
    "\n",
    "    # Run the pipeline\n",
    "    user_ids, user_likes, hashed_jaccard_sim = minhash_pipeline(new_data)\n",
    "       \n",
    "    # Get index of our user_id and distances for that user\n",
    "    idx_user_id = user_ids.index(user_id)\n",
    "    user_dists = hashed_jaccard_sim[idx_user_id,:].tolist()\n",
    "    \n",
    "    # Sort the distances in decreasing order of similarity (highly similar users: their likes are good recommendations)\n",
    "    sorted_user_dists = zip(user_ids, user_dists)\n",
    "    sorted_user_dists = sorted(sorted_user_dists, key=lambda a: a[1], reverse=True)\n",
    "    \n",
    "    # Nearest k neighbours are usually the range 0:k\n",
    "    # But, the user itself is also in the sorted distances, so we need to skip over index 0, which makes it 1:k+1\n",
    "    nearest_k_users = sorted_user_dists[1:k+1]\n",
    "    \n",
    "    if Config.do_print: print(f\"Retrieved {len(nearest_k_users)}-nearest users for user {user_id}\")\n",
    "    \n",
    "    # Get the set of likes for our user_id\n",
    "    user_like_set = np.array(list(user_likes[idx_user_id]))\n",
    "    \n",
    "    # Get the set of likes for our nearest-k\n",
    "    nearest_likes = set()\n",
    "    for i in range(len(nearest_k_users)):\n",
    "        neighbour_id_and_distance = sorted_user_dists[i]\n",
    "        neighbour_id = neighbour_id_and_distance[0]\n",
    "        idx_neighbour = user_ids.index(neighbour_id)\n",
    "        neighbour_like_set = np.array(list(user_likes[idx_neighbour]))\n",
    "        for like in neighbour_like_set:\n",
    "            nearest_likes.add(like)\n",
    "    \n",
    "    # Remove ids our user already liked\n",
    "    # Remaining are liked users for our nearest neighbours that our user hasn't liked yet\n",
    "    # These are good recommendations for our user, as they are more likely to also like them, because similar users to our user also liked these.\n",
    "    recommendations = nearest_likes.difference(user_like_set)\n",
    "    \n",
    "    print_k_neighbours(user_likes, user_ids, user_dists, 3)\n",
    "    \n",
    "    print()\n",
    "    print(\"===== Output =====\")\n",
    "    print(f\"Found {len(recommendations)} recommendations for user {user_id} from their {len(nearest_k_users)} nearest neighbours' likes.\")\n",
    "    print(f\"{len(nearest_likes)} neighbours' likes were under consideration, but {len(nearest_likes)-len(recommendations)} intersecting likes with user {user_id} were removed.\")\n",
    "    print(f\"The recommendations are:\")\n",
    "    print(f\" - {sorted(list(recommendations))}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "recommendations = mh_get_recommendations_for_demo_user(train_data, user_id=4444, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the demo user with 10 likes that user 80 also liked finds user 80 as one of it's three nearest neighbours, and the pipeline is able to recommend further users that this new user might like given on their approximated hashed similarity to other users' likesets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random bits of code\n",
    "Not necessarily applicable to the assignment, but preserved for posterity\n",
    "\n",
    "You can collapse the \"*Random bits of code*\" header to hide all the below.\n",
    "\n",
    "## The below code is not for grading\n",
    "\n",
    "## Stop reading unless you're morbidly curious in discarded code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old functions\n",
    "Some functions turned out to be unnecessary. They are preserved here in case they are needed anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will build our user-user likes matrix\n",
    "# We want a matrix of n x n, where the columns are the users, and the rows are their like record for the other users\n",
    "# - n       : number of users\n",
    "# - cells   : number of likes from col_user to row_user, -inf if missing, -1 if not liked\n",
    "\n",
    "\n",
    "# We need a count of True likes for (from_user, to_user) pairs\n",
    "def get_user_pair_values(\n",
    "    df: pd.DataFrame, value_of_like=1, value_of_dislike=0\n",
    ") -> dict[Tuple[int, int], int]:\n",
    "    \"\"\"\n",
    "    Get the count of likes for every pair of users in a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (DataFrame) : Dataframe containing columns `user_from_id : int`, `user_to_id : int` and `is_like : bool`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    user_pair_likes : dict[(int, int), int]\n",
    "        Dict of the count of likes for the pair (`from_user`, `to_user`)\n",
    "    \"\"\"\n",
    "    result_dict: dict[Tuple[int, int], int] = dict()\n",
    "\n",
    "    for i, row in train_data.iterrows():\n",
    "        from_id: int = row[\"user_from_id\"]\n",
    "        to_id: int = row[\"user_to_id\"]\n",
    "        is_like: bool = row[\"is_like\"]\n",
    "\n",
    "        tuple = (from_id, to_id)\n",
    "\n",
    "        relation_value = value_of_like if is_like else value_of_dislike\n",
    "\n",
    "        if tuple in result_dict:\n",
    "            result_dict[tuple] += relation_value\n",
    "        else:\n",
    "            result_dict[tuple] = relation_value\n",
    "    return result_dict\n",
    "\n",
    "user_likes_dict = get_user_pair_values(train_data, value_of_like=1, value_of_dislike=0)\n",
    "\n",
    "def get_max_dict_tuple_key(dictionary: dict[Tuple[int, int], Any]) -> int:\n",
    "    \"\"\"\n",
    "    For an input dictionary that has keys of (int, int), return the max value on either side of the tuple.\n",
    "    \"\"\"    \n",
    "    indices = np.array(list(dictionary.keys()))\n",
    "\n",
    "    m, n = indices.max(0) # max extents of indices to decide o/p array\n",
    "    m = max(m, n)\n",
    "    return m\n",
    "\n",
    "def dict_to_matrix(dictionary: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a matrix from an input dict, where the input dict has a pair of indices as keys and as values the value for that index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary (dict) : Dictionary containing (`int`, `int`) tuple as keys, and values of the same datatype as matrix values for the key indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : ndarray\n",
    "        Matrix with dictionary values at key int-pair (m, n) indices\n",
    "    \"\"\"\n",
    "    indices = np.array(list(dictionary.keys()))\n",
    "    values = np.array(list(dictionary.values()))\n",
    "\n",
    "    max_id = get_max_dict_tuple_key(dictionary)\n",
    "    result = np.full((max_id + 1, max_id + 1), float(\"-inf\"))\n",
    "    result[indices[:, 0], indices[:, 1]] = values\n",
    "\n",
    "    return result\n",
    "\n",
    "# Used to calculate the boolean matrix for mh_similarity_matrix, but mh_similarity_matrix was too slow.\n",
    "# Found a way to do it with scipy after... *looks on watch*... 3 hours of debugging. Feest!\n",
    "def mh_jaccard_matrix(data):\n",
    "    \"\"\"_summary_\n",
    "    Calculates a Jaccard matrix for the data\n",
    "    The matrix is m x n\n",
    "    - m : number of columns in the data\n",
    "    - n : number of unique elements in the data\n",
    "    \n",
    "    It fills the matrix with True and False based on if a column of the data contains an element of the data\n",
    "    \n",
    "    Example:\\n\n",
    "    [[0, 3]\n",
    "     [1, 4]\n",
    "     [2, 5]]\n",
    "     \n",
    "    Returns:  \n",
    "    0: [[T, F]\n",
    "    1:  [T, F]\n",
    "    2:  [T, F]\n",
    "    3:  [F, T]\n",
    "    4:  [F, T]\n",
    "    5:  [F, T]]\n",
    "    \n",
    "    Likewise:\\n\n",
    "    [[0, 7]\n",
    "     [1, 1]]\n",
    "     \n",
    "    Returns:  \n",
    "    0: [[T, F]\n",
    "    1:  [T, T]\n",
    "    7:  [F, T]]\n",
    "    \"\"\"\n",
    "    unique_entries = np.unique(data)\n",
    "    \n",
    "    n = data.shape[1]\n",
    "    m = unique_entries.shape[0]\n",
    "    jaccard_matrix = np.zeros((m, n), dtype=bool)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(f\"Number of unique entries: {unique_entries.size}\")\n",
    "        print(f\"unique_entries: {unique_entries}\")\n",
    "        print(unique_entries[0:101])\n",
    "    \n",
    "    for i in range(data.shape[1]):\n",
    "        col = np.isin(unique_entries, data[:,i])\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     print()\n",
    "        #     print(col)\n",
    "        #     print()\n",
    "        \n",
    "        jaccard_matrix[:,i] = col\n",
    "    \n",
    "    # if DEBUG: print(jaccard_matrix)\n",
    "    return jaccard_matrix\n",
    "\n",
    "# Too slow! Found a scipy way to do it\n",
    "def mh_similarity_matrix(data):\n",
    "    n = data.shape[1]\n",
    "    out = np.eye(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        X = data[:,i]\n",
    "        for j in range(n):\n",
    "            Y = data[:,j]\n",
    "            out[i,j] = simple_jaccard(X, Y)\n",
    "    \n",
    "    # Take the inverse, because we want the similarity, not the dissimilarity scipy.spatial.distance.jaccard computes\n",
    "    out = np.abs(out - 1)\n",
    "    return np.abs(out - 1)\n",
    "\n",
    "# Alternative to make a sparse matrix if necessary for RAM usage\n",
    "# def get_user_to_user_like_matrix_3(A: dict) -> csr_matrix:\n",
    "#     idx = np.array(list(A.keys()))\n",
    "#     val = np.array(list(A.values()))\n",
    "\n",
    "#     m, n = idx.max(0) + 1  # max extents of indices to decide o/p array\n",
    "#     m = max(m, n)\n",
    "#     out = csr_matrix((val, (idx[:, 0], idx[:, 1])), shape=(m, m))\n",
    "#     return out\n",
    "\n",
    "\n",
    "user_likes_matrix = dict_to_matrix(user_likes_dict)\n",
    "\n",
    "# print(user_pair_values)\n",
    "print(\n",
    "    \"user_like_matrix\",\n",
    "    f\"shape       : {user_likes_matrix.shape}\",\n",
    "    f\"dtype       : {user_likes_matrix.dtype}\",\n",
    "    f\"size        : {user_likes_matrix.size}\",\n",
    "    f\"size (bytes): {user_likes_matrix.size * user_likes_matrix.itemsize}\",\n",
    "    f\"size (bytes): {user_likes_matrix.nbytes}\",\n",
    "    sep=\"\\n  - \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeated_pair(from_id, to_id, index):\n",
    "    for i, row in train_data.iterrows():\n",
    "        other_from_id = row[\"user_from_id\"]\n",
    "        other_to_id = row[\"user_to_id\"]\n",
    "        if from_id == other_from_id and other_to_id == to_id and not i == index:\n",
    "            print(f\"Equality found: {i}: {other_from_id}, {other_to_id}\")\n",
    "\n",
    "\n",
    "def check_reversed_pair(from_id, to_id):\n",
    "    for i, row in train_data.iterrows():\n",
    "        other_from_id = row[\"user_from_id\"]\n",
    "        other_to_id = row[\"user_to_id\"]\n",
    "        if from_id == other_to_id and other_from_id == to_id:\n",
    "            print(f\"Equality found: {i}: {other_from_id}, {other_to_id}\")\n",
    "\n",
    "\n",
    "tuple_set = set()\n",
    "tuple_dict = dict()\n",
    "duplicate_set = set()\n",
    "count = 0\n",
    "\n",
    "for i, row in train_data.iterrows():\n",
    "    from_id = row[\"user_from_id\"]\n",
    "    to_id = row[\"user_to_id\"]\n",
    "\n",
    "    tuple = (from_id, to_id)\n",
    "    if tuple in tuple_set:\n",
    "        # print(f\"{i}: {from_id},{to_id}\")\n",
    "        duplicate_set.add(tuple)\n",
    "        count += 1\n",
    "    tuple_set.add(tuple)\n",
    "\n",
    "    if tuple in tuple_dict:\n",
    "        tuple_dict[tuple] = +1\n",
    "    else:\n",
    "        tuple_dict[tuple] = 0\n",
    "\n",
    "print(count)\n",
    "count = 0\n",
    "\n",
    "for key in tuple_dict:\n",
    "    if tuple_dict[key] > 0:\n",
    "        count += 1\n",
    "        # print(key, tuple_dict[key])\n",
    "\n",
    "# a, b, True, ---\n",
    "# a, b, False, ---\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n",
    "# for tuple in duplicate_set:\n",
    "#     self_from_id = tuple[0]\n",
    "#     self_to_id = tuple[1]\n",
    "#     for (i, row) in train_data.iterrows():\n",
    "#         from_id = row['user_from_id']\n",
    "#         to_id = row['user_to_id']\n",
    "#         # is_like = row['is_like']\n",
    "#         # is_match = row['is_match']\n",
    "\n",
    "#         print(f\"{i}: {from_id}, {to_id}\")\n",
    "#         check_repeated_pair(from_id, to_id, i)\n",
    "#         print(\"\")\n",
    "\n",
    "#         print(\"\")\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "for i, row in train_data.iterrows():\n",
    "    if row[\"is_like\"]:\n",
    "        a += 1\n",
    "    b += 1\n",
    "\n",
    "print(a, \"/\", b, \"=\", a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    W = np.array([[2, 2, 2, 2], [4, 4, 4, 4]])\n",
    "    H = np.array([[1, 2, 3, 4], [4, 4, 2, 0]])\n",
    "    \n",
    "    e = 1e-15\n",
    "    print(e)\n",
    "    \n",
    "    print(W)\n",
    "    print(H)\n",
    "    print(W/(H+e))\n",
    "    print(W * H)\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [{1, 2, 3, 4}, {1}, {4, 5}, {1, 2, 3}, {1}]\n",
    "space = set().union(*ids)\n",
    "a = sorted(space)\n",
    "\n",
    "print(np.full((0, 0), sys.maxsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross-validate\n",
    "# n_chunks = 10\n",
    "# chunks = np.array_split(train_data, n_chunks)\n",
    "\n",
    "# for i in range(n_chunks):\n",
    "#     # Select current test chunk and synthesize train chunk from remaining chunks\n",
    "#     test_chunk = chunks[i]\n",
    "#     train_chunk = []\n",
    "#     for j in range(n_chunks):\n",
    "#         if i!=j: train_chunk.append(chunks[j])\n",
    "#     # Apply all of the above code\n",
    "#     print(train_chunk)\n",
    "#     generated_matrix = nmf(get_user_interactions(pd.DataFrame(train_chunk)), 5, 100, 1e-3)\n",
    "#     result_list = apply_matrix(generated_matrix,test_chunk)\n",
    "#     score = 0\n",
    "#     for j, row in test_chunk:\n",
    "#         if row['is_like'] == result_list[j]: score+=1\n",
    "#     print(100*score/len(test_chunk),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
