{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Custom config class to globally change some parameters\n",
    "class Config:\n",
    "    run_tests: bool\n",
    "    do_print: bool\n",
    "\n",
    "Config.run_tests = True\n",
    "Config.do_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Run All cells now before reading on\n",
    "\n",
    "## For peer-reviewing the report, scroll down to the H1 header \"Start of Report\"\n",
    "\n",
    "You can also collapse the headers **Ingest data** and **Function implementations** to get there quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function implementations\n",
    "\n",
    "The functions below are for reference only. The WebLab functions are to be graded. These may have changed to accomodate our pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral embeddings\n",
    "### Provided design specification\n",
    "Implement the computation of [spectral embeddings](https://en.wikipedia.org/wiki/Spectral_clustering#Algorithms) for **undirected graphs**.\n",
    "\n",
    "- The function to implement is `compute_spectral_embeddings`.\n",
    "- The input is an undirected [networkx.Graph](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    "- The node IDs in this graph are integers from `0` to `n`.\n",
    "- The output should be a numpy array of shape `(num_nodes, dim)`. Each row corresponds to a node representation. Rows should be ordered by node ID (ascending).\n",
    "- The Laplacian should not be normalized.\n",
    "- You have to use [numpy.linalg.eigh](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) to compute eigenvalues and eigenvectors in order for the tests to work.\n",
    "- This task focuses on computing the node representations. You do not need to run an actual clustering algorithm (e.g., k-means).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral embeddings implementation and tests\n",
    "\n",
    "def compute_spectral_embeddings(graph: nx.Graph, dim: int) -> np.ndarray:\n",
    "    \"\"\"Perform spectral clustering on the graph and compute low-dimensional node representations.\n",
    "    Does not normalize the Laplacian.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph.\n",
    "        dim (int): The dimension of representations. This corresponds to the number of eigenvectors used.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Node representations (sorted by node ID, ascending), shape (num_nodes, dim).\n",
    "    \"\"\"\n",
    "    adjacency_matrix = nx.to_numpy_array(graph, nodelist=sorted(graph.nodes))\n",
    "\n",
    "    # make sure the matrix is symmetric\n",
    "    assert (adjacency_matrix == adjacency_matrix.T).all()\n",
    "\n",
    "    result = None\n",
    "\n",
    "    # START ANSWER\n",
    "\n",
    "    # END ANSWER\n",
    "\n",
    "    return result\n",
    "\n",
    "if Config.run_tests:\n",
    "    import unittest\n",
    "    \n",
    "    class TestComponents(unittest.TestCase):\n",
    "        def setUp(self):\n",
    "            self.simple_graph = nx.Graph(\n",
    "                [(0, 3), (1, 3), (2, 4), (3, 5), (3, 6), (4, 6), (5, 6), (5, 4)]\n",
    "            )\n",
    "\n",
    "        def test_spectral_embeddings(self):\n",
    "            emb = compute_spectral_embeddings(self.simple_graph, 3)\n",
    "            np.testing.assert_almost_equal(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [3.77964473e-01, -4.49723806e-01, 7.07106781e-01],\n",
    "                        [3.77964473e-01, -4.49723806e-01, -7.07106781e-01],\n",
    "                        [3.77964473e-01, 6.59857436e-01, -1.66533454e-15],\n",
    "                        [3.77964473e-01, -2.18583490e-01, 1.11022302e-16],\n",
    "                        [3.77964473e-01, 3.20716714e-01, 6.59900940e-16],\n",
    "                        [3.77964473e-01, 6.87284763e-02, 1.14927529e-15],\n",
    "                        [3.77964473e-01, 6.87284763e-02, 1.26029760e-15],\n",
    "                    ]\n",
    "                ),\n",
    "                emb,\n",
    "            )\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walks implementation\n",
    "### Provided design specification\n",
    "Implement [random walks](https://en.wikipedia.org/wiki/Random_walk). You can assume that the graph does not have weighted edges.\n",
    "\n",
    "The function to implement is `random_walks`.\n",
    "The input is an undirected [networkx.Graph](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    "The output should be a numpy array, shape `(num_nodes * num_walks, walk_length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random walks implementation and tests\n",
    "def random_walks(graph: nx.Graph, num_walks: int, walk_length: int) -> np.ndarray:\n",
    "    \"\"\"Perform random walks on an unweighted graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph.\n",
    "        num_walks (int): The number of random walks for each node.\n",
    "        walk_length (int): The number of nodes in a random walk.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The random walks, shape (n_nodes * num_walks, walk_length)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    # START ANSWER\n",
    "    # TODO Change to np.zeros later\n",
    "    result = np.full((len(graph)*num_walks, walk_length), -1)\n",
    "    # randomizer = np.random.default_rng()\n",
    "\n",
    "    def random_walk(graph: nx.Graph, starting_node: int, walk_length: int) -> np.ndarray:\n",
    "        walk = np.zeros(walk_length)\n",
    "        walk[0] = starting_node\n",
    "        current_node = starting_node\n",
    "        for i in range(walk_length - 1):\n",
    "            neighbors = list(graph.adj[current_node])\n",
    "            current_node = np.random.choice(neighbors)\n",
    "            walk[i + 1] = current_node\n",
    "        return walk\n",
    "            \n",
    "    for node in graph.nodes:\n",
    "        for i in range(num_walks):\n",
    "            walk = random_walk(graph, node, walk_length)\n",
    "            # print(type(node))\n",
    "            # print(node)\n",
    "            index = num_walks*node + i\n",
    "            # print(f\"node={node}, index={index}\")\n",
    "            result[index, :] = walk\n",
    "            print(result)\n",
    "    # END ANSWER\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "if Config.run_tests:\n",
    "    import unittest\n",
    "\n",
    "    class TestComponents(unittest.TestCase):\n",
    "        def setUp(self):\n",
    "            self.simple_graph = nx.Graph(\n",
    "                [(0, 3), (1, 3), (2, 4), (3, 5), (3, 6), (4, 6), (5, 6), (5, 4)]\n",
    "            )\n",
    "\n",
    "        def test_random_walks(self):\n",
    "            result = random_walks(self.simple_graph, 2, 5)\n",
    "            self.assertEqual((7 * 2, 5), result.shape)\n",
    "            for n in result.flatten():\n",
    "                self.assertIn(n, self.simple_graph.nodes)\n",
    "\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir_name: str, file_name: str):\n",
    "    \"\"\"Read the medium articles with lists\n",
    "\n",
    "    Args:\n",
    "        dir_name (str): Root directory of the medium title files and lists.\n",
    "\n",
    "    Returns:\n",
    "        final_data: merged dataframes with articles and lists\n",
    "    \"\"\"\n",
    "\n",
    "    final_data = pd.read_csv(dir_name+\"/\"+file_name+\".csv\")\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv(\"data/pipeline_assignment_data/full_data_without_labels.csv\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(\"data/pipeline_assignment_data\",\"train\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data(\"data/pipeline_assignment_data\",\"test\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "multilabel_binarizer = LabelEncoder()\n",
    "multilabel_binarizer.fit(train[\"labels\"])\n",
    "\n",
    "Y = multilabel_binarizer.transform(train[\"labels\"])\n",
    "texts = [x[0]+\" \" + x[1] for x in zip(train.title,train.subtitle)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(final_data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "all_texts = [x[0]+\" \" + x[1] for x in zip(final_data.title,final_data.subtitle)]\n",
    "\n",
    "word2vec_model = Word2Vec([text.split(\" \") for text in all_texts], vector_size=128, window=10, epochs=30, sg=1, workers=4,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "embeddings = []\n",
    "\n",
    "for text in texts:\n",
    "    embeddings.append(np.mean([word2vec_model.wv[word] for word in text.split(\" \")], axis=0))\n",
    "print(np.vstack(embeddings).shape)\n",
    "X_word2vec = np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_word2vec,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "text_texts = [x[0]+\" \" + x[1] for x in zip(test.title,test.subtitle)]\n",
    "\n",
    "for text in text_texts:\n",
    "    test_embeddings.append(np.mean([word2vec_model.wv[word] for word in text.split(\" \")], axis=0))\n",
    "print(np.vstack(test_embeddings).shape)\n",
    "X_word2vec_test = np.vstack(test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_word2vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = multilabel_binarizer.transform(test[\"labels\"])\n",
    "\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import List,Dict\n",
    "\n",
    "\n",
    "def get_edges(data: pd.DataFrame, nodes) -> List:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of edges\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "        nodes (dict): List of nodes\n",
    "    Returns:\n",
    "            nodes: dict (nodeid: article title)\"\"\"\n",
    "    edges = []    \n",
    "    ## START\n",
    "    \n",
    "    ##END\n",
    "    return edges\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(data: pd.DataFrame) -> Dict:\n",
    "    \"\"\" Given the dataframe with articles and lists return the set of nodes\n",
    "        Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "    Returns:\n",
    "        nodes: dict (nodeid: article title)\"\"\"\n",
    "    nodes = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if index not in nodes:\n",
    "            nodes[index] = row[\"title\"]\n",
    "    print(len(nodes))\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def form_graph(data: pd.DataFrame) -> nx.Graph:\n",
    "    \"\"\"Forms graph from medium article dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The medium dataset\n",
    "\n",
    "    Returns:\n",
    "        G (nx.Graph): The graph.\n",
    "\n",
    "       \"\"\"\n",
    "    texts = [x[0]+\" \" + x[1] for x in zip(data.title,data.subtitle)]\n",
    "    nodes = get_nodes(data)\n",
    "    edges = get_edges(data, nodes)\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(list(nodes.keys()))\n",
    "    graph.add_edges_from(edges)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = form_graph(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(graph.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find isolated nodes with no neighbors before random walks\n",
    "isolated = [x for x in nx.isolates(graph)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walks(G: nx.Graph, num_walks: int, walk_length: int, isolated: List) -> np.ndarray:\n",
    "    \"\"\"Perform random walks on the graph.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The graph.\n",
    "        num_walks (int): The number of random walks for each node.\n",
    "        walk_length (int): The number of nodes in a random walk.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The random walks, shape (n_nodes * num_walks, walk_length)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ### START\n",
    "    \n",
    "    ## END\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = random_walks(graph, 8, 15,isolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def fit_node2vec(walks: np.ndarray, vector_size: int, window: int, epochs: int) -> Word2Vec:\n",
    "    \"\"\"Train a Node2Vec model on random walks. Uses the GenSim Word2Vec implementation.\n",
    "\n",
    "    Args:\n",
    "        walks (np.ndarray): The random walks.\n",
    "        vector_size (int): Node representation size.\n",
    "        window (int): Window width.\n",
    "        epochs (int): Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        Word2Vec: The trained model.\n",
    "    \"\"\"\n",
    "   ### START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_node2vec(walks, 128, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = {doc: model.dv[doc] for doc in model.dv.index_to_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodeids(data: pd.DataFrame):\n",
    "    nodes = {}\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"index\"] not in nodes:\n",
    "            nodes[row[\"index\"]] = row[\"title\"]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = get_nodeids(train)\n",
    "test_nodes = get_nodeids(test)\n",
    "train_nodes = list(train_nodes.keys())\n",
    "test_nodes = list(test_nodes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_train_n2v = np.hstack(\n",
    "    (\n",
    "        X_word2vec, np.array([embeddings[str(x)] if x not in isolated else X_word2vec[idx] for idx, x in enumerate(train_nodes)  ], dtype=np.float32),\n",
    "\n",
    "    )\n",
    ")\n",
    "X_test_n2v =np.hstack(\n",
    "    (\n",
    "        X_word2vec_test,\n",
    "        np.array([embeddings[str(x)] if x not in isolated else X_word2vec_test[idx] for idx, x in enumerate(test_nodes)  ], dtype=np.float32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_n2v.shape,X_train_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train_n2v,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_n2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only node2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_train_n2v =  np.array([embeddings[str(x)] if x not in isolated else np.zeros((128)) for idx, x in enumerate(train_nodes)  ], dtype=np.float32)\n",
    "\n",
    "X_test_n2v =np.array([embeddings[str(x)] if x not in isolated else np.zeros((128)) for idx, x in enumerate(test_nodes)  ], dtype=np.float32)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train_n2v,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test_n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.f1_score(Y_test, predictions,average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
